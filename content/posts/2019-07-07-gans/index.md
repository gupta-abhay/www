---
order: 1
path: '/blog/gans-reading-list'
title: 'Generative Adversarial Networks - An essential reading list'
subtitle: 'compiled reading resources'
published: true
date: '20190707'
type: 'post'
keywords: 'survey, gans'
tags:
    - machine learnings
year: 2019
---

Generative Adversarial Networks (GANs) have grown into a very widely explored field of research. It is being used for many applications such as the generation of hyper-realistic natural images, in addition to style-transfer, image super-resolution, natural language generation, music generation, medical data generation, and physical modeling. In sum, these applications represent a major advance in the capabilities of machine intelligence and will have significant and immediate practical consequences. Even more promisingly, in the long run, deep generative models are a potential method for developing rich representations of the world from unlabeled data, similar to how humans develop complex mental models, in an unsupervised way, directly from sensory experience. 

However, since GANs have exploded, there have been a large number of papers that have used GANs for multiple purposes. I have compiled some of the most relevant papers, according to me, for understanding the theory and some applications of GANs. The `theory` papers have been compiled in order of reading to best understand the different concepts of GANs. The other two sections can be read in any order as they are additional papers that show some impressive applications or provide a more supporting theory for a deeper understanding.

### Theory

* [Generative Adversarial Networks](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)
* [Conditional GAN](https://arxiv.org/pdf/1411.1784.pdf)
* [Laplacian Pyramid of GANs (LAPGANs)](https://arxiv.org/pdf/1506.05751.pdf)
* [Deep Convolutional GANs (DCGANs)](https://arxiv.org/pdf/1511.06434.pdf)
* [Improved Techniques for Training GANs](https://arxiv.org/pdf/1606.03498.pdf)
* [A Note about Inception Score](https://arxiv.org/pdf/1801.01973.pdf)
* [Fr√©chet Inception Distance (FID)](https://papers.nips.cc/paper/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium.pdf)
* [Mode Regularized GANs](https://arxiv.org/pdf/1612.02136.pdf)
* [Wassterstein GANs (WGANs)](https://arxiv.org/pdf/1701.07875.pdf)


### Vision based Applications

* [Pix2Pix](https://arxiv.org/pdf/1611.07004.pdf)
* [CycleGANs](https://arxiv.org/pdf/1703.10593.pdf)
* [Progressive Growing of GANs](https://arxiv.org/pdf/1710.10196.pdf)
* [BigGANs](https://arxiv.org/pdf/1809.11096.pdf)
* [StyleGANs](https://arxiv.org/pdf/1812.04948.pdf)
* [GauGANs](https://arxiv.org/pdf/1903.07291.pdf)
* [BigBiGANs](https://arxiv.org/pdf/1907.02544.pdf)

### Additional Theory Papers

* [Intriguing properties of neural networks](https://arxiv.org/pdf/1312.6199.pdf)
* [On Distinguishability Criteria for Estimating Generative Models](https://arxiv.org/pdf/1412.6515.pdf)
* [Explaining and Harnessing Adversarial Examples](https://arxiv.org/pdf/1412.6572.pdf)
* [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/pdf/1512.00567.pdf)
* [Towards Principled Unsupervied Learninig](https://arxiv.org/pdf/1511.06440.pdf)
* [Spectral Normalization for GANs](https://openreview.net/pdf?id=B1QRgziT-)
* [Towards Principled Methods for Training GANs](https://arxiv.org/pdf/1701.04862.pdf)
* [Variational Approaches for Auto-Encoding GANs](https://arxiv.org/pdf/1706.04987.pdf)
* [Are GANs Created Equal? A Large-Scale Study](https://arxiv.org/pdf/1711.10337.pdf)
* [An Emperical Study of Evaluation Metrics of GANs](https://arxiv.org/pdf/1806.07755.pdf)
* [GAN Dissection: Visualizing and Understanding Generative Adversarial Networks](https://gandissect.csail.mit.edu/)

Happy readings !!!